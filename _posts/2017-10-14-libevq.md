---
layout: post
title: Event Queue Library
---

## Introduction

Recently I've been hacking on some code that needs to handle thousands of TCP connections running on Linux, but I also wanted to run it on my development laptop which runs macOS. The problem is that [select(2)](http://man7.org/linux/man-pages/man2/select.2.html) and [poll(2)](http://man7.org/linux/man-pages/man2/poll.2.html) which are implemented for both OSes not scalable.

## A scalable and explicit event delivery mechanism for UNIX

Back in 1999 there was a [paper](https://www.usenix.org/legacy/event/usenix99/full_papers/banga/banga.pdf) describing `select()` and `poll()` scalability issues. Authors suggested almost perfect new interface. They missed one important thing though. You could associate event with file descriptor, but not with an abstract identifier (such as pointer). Later something went wrong and two different event notification mechanisms were developed: [epoll(7)](http://man7.org/linux/man-pages/man7/epoll.7.html) and [kqueue(2)](https://www.freebsd.org/cgi/man.cgi?query=kqueue). Are they really that different? What about [libevent](http://libevent.org/)?

## Libevent

Event loops. Callbacks. Your application should be designed around that. What if I have other design preferences? Is there something else around? Yes! [libev](http://software.schmorp.de/pkg/libev.html), [libuev](https://github.com/troglobit/libuev), [picoev](https://github.com/kazuho/picoev), but they all have been inspired by libevent and share the same API problems. To be more specific: they all use callbacks as an event delivery mechanism.

## Finite-state machine

While stackless coroutines are still not available in C/C++ my favorite way to handle sockets is [FSM](https://en.wikipedia.org/wiki/Finite-state_machine). The code could be as simple as:

```c
struct state state;
bool done = state_init(&state);
while (!done)
  done = state.handler(&state);
state_fini(&state);
```
with following handlers:
```c
bool receiver(struct state *state) {
  if (recv(...) <= 0) return (true);
[skip]
  /* ready to send */
  state->handler = sender;
  return (false);
}

bool sender(struct state *state) {
[skip]
  /* ready to receive */
  state->handler = receiver;
  return (false);
}
```

I prefer this implementation over [state transition table](https://en.wikipedia.org/wiki/State_transition_table).

So, we want to wait for read or write events associated with state.

## Define proper API

```c
evq_t evq_create(int nevents);
void evq_destroy(evq_t evq);
```

Where _nevents_ argument defines how much memory will be allocated internally. It is not the maximum number of events.

```c
int evq_add_read(evq_t evq, int fd, void *data);
int evq_add_write(evq_t evq, int fd, void *data);
int evq_set_read(evq_t evq, int fd, void *data);
int evq_set_write(evq_t evq, int fd, void *data);
int evq_del(evq_t evq, int fd, void *data);
```

Sadly enough, but [epoll doesn't have "add or modify" operation](https://groups.google.com/forum/#!topic/fa.linux.kernel/S1Fj2L2ZO7Y), so we need 4 functions instead of 2 to control interested events. `evq_del()` is not really needed because file descriptor will be automatically removed on last `close()`.

```c
int evq_wait(evq_t evq, int msec);
void *evq_next(evq_t evq);
```

Let's modify our FSM loop to handle multiple states:

```c
while (evq_wait(evq, -1) != -1) {
  while ((state = evq_next(evq)) != NULL) {
    done = state->handler(state);
    if (done)
      state_fini(state);
  }
}
```
and handlers:
```c
bool receiver(struct state *state) {
[skip]
  state->handler = sender;
  return (evq_set_write(evq, state->fd, state) != 0);
}

bool sender(struct state *state) {
[skip]
  state->handler = receiver;
  return (evq_set_read(evq, state->fd, state) != 0);
}
```

You can find example [here](https://github.com/z0nt/libevq/blob/master/examples/echo.c).

#### Note

Not `epoll()`, nor `kqueue()` have interface to cancel pending events. You need to maintain some data structure to be able to iterate over pending events and clean them up.

## Timers

Obviously, you want to have timeouts, but I encourage you not to use system timers for every connection. There are at least two problems with them. First of all it is overhead of passing data back and forth between application and kernel. Also kernel needs to allocate memory for each and every timer and manages them. Second, you stuck with kernel implementation, while in application you have a [choice](https://blog.acolyer.org/2015/11/23/hashed-and-hierarchical-timing-wheels/). What you really need is to be able to get closest timeout and pass it to `evq_wait()`.

## Conclusion

It is possible to define common API for `epoll()` and `kqueue()` with no CPU and memory overhead.

The code can be found [here](https://github.com/z0nt/libevq).
